# LLM 설정 파라미터 설명

## 현재 코드에서 사용하는 설정

```python
def call_llm(prompt, max_tokens=4000, temperature=0.7):
    client = anthropic.Anthropic(api_key=api_key, timeout=120.0)
    message = client.messages.create(
        model='claude-sonnet-4-20250514',
        max_tokens=max_tokens,
        temperature=temperature,
        messages=[{"role": "user", "content": full_prompt}]
    )
```

---

## 📊 각 파라미터 의미

### 1. **`model` (모델 버전)**
```python
model='claude-sonnet-4-20250514'
```

**의미:**
- 사용할 AI 모델의 버전을 지정
- `claude-sonnet-4-20250514`: Claude Sonnet 4 모델의 2025년 5월 14일 버전

**다른 모델 옵션:**
- `claude-3-5-sonnet-20241022`: Claude 3.5 Sonnet (더 최신 버전)
- `claude-3-opus-20240229`: Claude 3 Opus (가장 강력하지만 느림)
- `claude-3-haiku-20240307`: Claude 3 Haiku (가장 빠르고 저렴)

**영향:**
- 모델에 따라 성능, 속도, 비용이 달라짐
- 최신 모델일수록 더 정확하고 긴 응답 가능

---

### 2. **`max_tokens` (최대 출력 토큰 수)**
```python
max_tokens=4000  # 기본값
```

**의미:**
- AI가 생성할 수 있는 **최대 출력 길이**를 제한
- **토큰(Token)**: 텍스트의 기본 단위 (한국어 약 1-2글자 = 1토큰, 영어 단어 1개 ≈ 1-2토큰)

**예시:**
- `max_tokens=4000` → 약 3,000-4,000자 정도의 한국어 텍스트 생성 가능
- `max_tokens=1000` → 약 750-1,000자 정도만 생성

**현재 프로젝트에서:**
- 분석 결과가 길어야 하므로 `4000`으로 설정
- 더 긴 분석이 필요하면 `8000` 또는 `16000`으로 증가 가능

**주의사항:**
- 토큰 수가 많을수록 비용 증가
- 실제 출력이 `max_tokens`보다 짧을 수도 있음 (AI가 적절히 마무리)

---

### 3. **`temperature` (온도 / 창의성 조절)**
```python
temperature=0.7  # 기본값
```

**의미:**
- AI 응답의 **일관성 vs 창의성**을 조절하는 파라미터
- 범위: `0.0` ~ `1.0` (일부 모델은 `2.0`까지 가능)

**값에 따른 효과:**

| Temperature | 효과 | 사용 예시 |
|------------|------|----------|
| **0.0** | 매우 일관적, 결정적 | 객관적 데이터 분석, 사실 기반 답변 |
| **0.3-0.5** | 일관적, 약간의 변동 | 비즈니스 리포트, 분석 문서 |
| **0.7** (현재) | 균형잡힌 창의성 | 전략 분석, 인사이트 도출 |
| **0.9-1.0** | 매우 창의적, 다양함 | 아이디어 발상, 창작 |

**현재 프로젝트에서 `0.7`을 사용하는 이유:**
- ✅ 데이터 기반 분석이지만 인사이트와 전략 제안도 필요
- ✅ 너무 낮으면(0.0-0.3) 단조롭고 기계적인 답변
- ✅ 너무 높으면(0.9-1.0) 일관성 없는 답변 가능

**조정 가이드:**
- 더 객관적/일관적인 분석이 필요 → `0.3-0.5`로 낮춤
- 더 창의적인 전략 제안이 필요 → `0.8-0.9`로 높임

---

### 4. **`timeout` (타임아웃)**
```python
timeout=120.0  # 120초 (2분)
```

**의미:**
- API 호출이 **응답을 기다리는 최대 시간** (초 단위)
- 이 시간 내에 응답이 없으면 에러 발생

**현재 설정:**
- `120.0초 = 2분`
- 복잡한 분석이므로 충분한 시간 확보

**조정 가이드:**
- 빠른 응답이 필요 → `60.0` (1분)
- 매우 복잡한 분석 → `180.0` (3분) 또는 `300.0` (5분)

**주의사항:**
- 타임아웃이 너무 짧으면 응답이 완료되기 전에 중단될 수 있음
- 타임아웃이 길어도 실제 응답이 빨리 오면 바로 반환됨

---

## 💡 실제 사용 예시

### 현재 설정으로 분석할 때:
```python
call_llm(prompt, max_tokens=4000, temperature=0.7)
```

**결과:**
- ✅ 최대 4,000 토큰(약 3,000-4,000자)의 분석 생성
- ✅ 일관성과 창의성의 균형잡힌 인사이트
- ✅ 2분 내 응답 완료

### 더 긴 분석이 필요할 때:
```python
call_llm(prompt, max_tokens=8000, temperature=0.7)
```

### 더 객관적인 분석이 필요할 때:
```python
call_llm(prompt, max_tokens=4000, temperature=0.3)
```

### 더 창의적인 전략이 필요할 때:
```python
call_llm(prompt, max_tokens=4000, temperature=0.9)
```

---

## 📈 토큰 사용량 추적

현재 코드는 토큰 사용량을 자동으로 추적합니다:

```python
# 각 LLM 호출마다 출력
[OK] LLM 응답 완료 (입력: 1,234 토큰, 출력: 567 토큰, 총: 1,801 토큰)

# 전체 분석 완료 후 출력
입력 토큰: 50,000 토큰
출력 토큰: 25,000 토큰
총 토큰: 75,000 토큰
```

**비용 계산:**
- Claude Sonnet 4 기준 (대략):
  - 입력: $3 / 1M 토큰
  - 출력: $15 / 1M 토큰
- 예시: 75,000 토큰 사용 시 약 $0.001 (약 1원)

---

## 🔧 설정 변경 방법

### 함수 호출 시 변경:
```python
# 더 긴 분석
analysis_response = call_llm(prompt, max_tokens=8000, temperature=0.7)

# 더 객관적인 분석
analysis_response = call_llm(prompt, max_tokens=4000, temperature=0.3)
```

### 함수 기본값 변경:
```python
def call_llm(prompt, max_tokens=8000, temperature=0.5):  # 기본값 변경
    ...
```

---

## 📝 요약

| 파라미터 | 현재 값 | 의미 | 조정 시 영향 |
|---------|--------|------|-------------|
| **model** | `claude-sonnet-4-20250514` | 모델 버전 | 성능, 속도, 비용 |
| **max_tokens** | `4000` | 최대 출력 길이 | 응답 길이, 비용 |
| **temperature** | `0.7` | 창의성 조절 | 일관성 vs 다양성 |
| **timeout** | `120.0` | 최대 대기 시간 | 에러 방지 |

**현재 설정은 비즈니스 분석에 적합한 균형잡힌 설정입니다!** ✅

